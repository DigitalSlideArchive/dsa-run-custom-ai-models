# Run Custom AI Models with DSA

You can build this functionality as follows:

```shell
docker build --force-rm -t dsarchive/run-custom-ai-models .
```

## What It Does

1. **Connect Custom AI Inference Models**: Users can seamlessly connect the custom AI inference models they create to the DSA platform.

2. **Configure AI Model Parameters**: Users have the flexibility to customize the parameters they send to their AI models by modifying this repository.

3. **Visualize AI Model Output**: The results generated by the AI models can be easily visualized within the DSA interface, and the outcomes can be saved to specific locations.

## How it Works

1. The provided Docker build command above constructs a Docker instance that includes both the user interface (UI) and backend necessary for integration with DSA.

2. Within the `ai-inference-model` folder, you'll find `run-ai-inferencing.py`, a FAST-API package designed to containerize your AI model.

3. After connecting your custom AI model, you can deploy the Docker container by running `docker build run-custom-ai-model`.

4. Once the Docker container is initialized, it will generate a URL. Copy this URL and paste it into the DSA user interface under "run-custom-ai-models."

5. Press the "Submit" button to initiate the analysis.