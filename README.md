
## What It Does

1. **Connect Custom AI Inference Models**: Users can seamlessly connect the custom AI inference models they create to the DSA platform.

2. **Configure AI Model Parameters**: Users have the flexibility to customize the parameters they send to their AI models by modifying this repository.

3. **Visualize AI Model Output**: The results generated by the AI models can be easily visualized within the DSA interface, and the outcomes can be saved to specific locations.

# Installation instructions

There are two components for this module:

1. **DSA Adapter**: Help users to deploy and run the AI adapter with Digital slide archive

Following are the steps needed to download and deploy the AI adapter

```shell
git pull https://github.com/DigitalSlideArchive/dsa-run-custom-ai-models.git
```
```shell
cd dsa-run-custom-ai-models
```
```shell
docker build --force-rm -t dsarchive/runcustomaimodels .
```
check if the docker image is present using the command
```shell
docker images
```
You should be able to locate the Docker image named `dsarchive/runcustomaimodels` in order to proceed. Once you've successfully confirmed its presence, proceed to add the AI adapter to the DSA platform by following the provided tutorial.

![Add AI adapter](./docs/media/add-docker-to-dsa.gif)

Once you receive a success message from the platform confirming that the Docker has been successfully added, you can validate this by navigating to the Digital Slide Archive platform and performing the following steps.

![Varify AI adapter](./docs/media/show-histomicstk.gif)


2. **AI API**: Deploys AI models seperately as a service. Users can add or remove AI models from this codebase.
Following are the steps needed to setup the API

```shell
cd aiInferenceModel
```

```shell
docker build -t dsarchive/aimodels .
```

```shell
docker run -t -d -p 8017:80 --name  aimodels dsarchive/aimodels 
```

You can varify the status of the docker image by doing.
```shell
docker images
```

## How it Works

1. The provided Docker build command above constructs a Docker instance that includes both the user interface (UI) and backend necessary for integration with DSA.

2. Within the `ai-inference-model` folder, you'll find `run-ai-inferencing.py`, a FAST-API package designed to containerize your AI model.

3. After connecting your custom AI model, you can deploy the Docker container by running `docker build run-custom-ai-model`.

4. Once the Docker container is initialized, You can choose and run the ai model required.

5. Press the "Submit" button to initiate the analysis.
